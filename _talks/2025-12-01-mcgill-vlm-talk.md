---
title: "Data Visualization Understanding with VLMs"
collection: talks
type: "Invited Talk"
permalink: /talks/2025-12-01-mcgill-vlm-talk
venue: "McGill University (McGill NLP Group)"
date: 2025-12-05
location: "Montreal, Quebec"
link: "https://mcgill-nlp.github.io/reading-group/fall-2025/ahmed-mazry/"
---

Data visualizations, such as charts, are central to how people analyze information and make informed decisions. Yet, automating their understanding (e.g., answering questions) remains challenging. While large vision-language models (VLMs) have advanced multimodal reasoning, they still underperform humans on data visualization tasks.

This talk explores our research journey in advancing multimodal chart understanding through benchmarks, training methods, and novel architectures. We introduce ChartQA and Chart-to-Text, our first large-scale benchmarks for question answering and summarization. Next, we present UniChart, a continual pretraining approach using chart-specific objectives and synthetic data to enhance visual math reasoning. Then, we reveal alignment gaps between vision and text modalities in VLMs and present AlignVLM, which bridges these gaps using a novel vision-text connector. We also discuss BigCharts, our synthetic chart generation technique that preserves real-world visual diversity. The talk concludes with open challenges and future directions for advancing VLMs' understanding of data visualizations and, more broadly, visually-situated language tasks.

